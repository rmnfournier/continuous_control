{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Reacher \n",
    "\n",
    "In this short Notebook, I will show how my implementation of Deep Deterministic Policy Gradient (DDPG) manages to solve Unity's Reacher environment. This project was carried out as part as Udacity's DRL nanodegree and some of the code are inspired from examples of the course.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by launching the environment. \n",
    "\n",
    "1) Download the environment \n",
    "Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n",
    "Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n",
    "Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n",
    "\n",
    "2) Indicate the location of the following file : \n",
    "Mac: \"path/to/Reacher.app\"\n",
    "\n",
    "Windows (x86): \"path/to/Reacher_Windows_x86/Reacher.exe\"\n",
    "\n",
    "Windows (x86_64): \"path/to/Reacher_Windows_x86_64/Reacher.exe\"\n",
    "\n",
    "Linux (x86): \"path/to/Reacher_Linux/Reacher.x86\"\n",
    "\n",
    "Linux (x86_64): \"path/to/Reacher_Linux/Reacher.x86_64\"\n",
    "\n",
    "Linux (x86, headless): \"path/to/Reacher_Linux_NoVis/Reacher.x86\"\n",
    "\n",
    "Linux (x86_64, headless): \"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"\n",
    "\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indicate the environment location in the location variable\n",
    "location = '../Project/Reacher_Linux/Reacher.x86_64'\n",
    "#no_graphics=True disables the graphics and speeds up the training\n",
    "env = UnityEnvironment(file_name=location,no_graphics=False)\n",
    "\n",
    "# Start the Brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Create the agent\n",
    "\n",
    "We now know that the state size is 33 and that the environment is waiting for a 4x1 dimensional action. We can use this information to create our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create an agent\n",
    "agent = Agent(state_size=33, action_size=4, random_seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note : skip this part if you don't want to train the agent, *\n",
    "The agent and the descriptions of the neural networks are located in ddpg_agent.py and model.py respectively. Here, we just create one agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start the training. We will output the average score over the last 5 episodes and the one over the 100 last ones. The environment is considered solved when the agent reaches an average score of 30 over the 100 last episodes. We will collect experience from 20 different agents, but only train one actor and one critic. Each agent will use the same weights for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def ddpg(n_episodes=550, print_every=5):\n",
    "    scores_last_episodes = deque(maxlen=print_every)\n",
    "    scores_target = deque(maxlen=100)\n",
    "    my_scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        agent.reset()                                          #Reset the agent \n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)              # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            agent.step(states,actions,rewards,next_states,dones) #Memorize and learn\n",
    "            scores += env_info.rewards                        # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "        score=np.mean(scores)   \n",
    "        scores_last_episodes.append(score)\n",
    "        scores_target.append(score)\n",
    "        my_scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score over 5 last episodes: {:.2f}\\t Average over 100 last episodes :{:.2f}'.format(i_episode, np.mean(scores_last_episodes),np.mean(scores_target)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_last_episodes)))\n",
    "        if(np.mean(scores_target)>30):\n",
    "            print('Environment Solved !')\n",
    "            break\n",
    "        \n",
    "    return my_scores\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "scores = ddpg()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch our agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        #agent.reset()\n",
    "scores = np.zeros(num_agents)         # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states,True)              # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    agent.step(states,actions,rewards,next_states,dones) #Memorize and learn\n",
    "    scores += env_info.rewards                        # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step            \n",
    "    if np.any(dones):\n",
    "        break \n",
    "print(np.mean(scores))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to save the weights !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the critic\n",
    "torch.save(agent.critic_local.state_dict(), \"critic.pth\")\n",
    "#Save the actor\n",
    "torch.save(agent.actor_local.state_dict(),\"actor.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching a trained agent the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and watch\n",
    "agent.critic_local.load_state_dict(torch.load(\"critic.pth\"))\n",
    "agent.actor_local.load_state_dict(torch.load(\"actor.pth\"))\n",
    "agent.critic_target.load_state_dict(torch.load(\"critic.pth\"))\n",
    "agent.actor_target.load_state_dict(torch.load(\"actor.pth\"))\n",
    "\n",
    "agent.epsilon=0\n",
    "agent.reset()\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)         # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states,False)              # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                        # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step            \n",
    "    if np.any(dones):\n",
    "        break \n",
    "print(np.mean(scores))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
